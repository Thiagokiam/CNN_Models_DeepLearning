{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEabO6uUKXbjS7/H1AoWEX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thiagokiam/CNN_Models_DeepLearning/blob/main/C%C3%B3pia_de_Model_TMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcOWAqW8oRKW"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import time, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImagemTensor(Dataset):\n",
        "  def __init__(self, dados, scaler_feat=None, scaler_label=None):\n",
        "\n",
        "    self.dados = dados\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    sample = self.dados[0][idx]\n",
        "    label  = self.dados[1][idx]\n",
        "\n",
        "    return sample, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dados[0])"
      ],
      "metadata": {
        "id": "cfIOKmsOornQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurando hiperparâmetros.\n",
        "args = {\n",
        "    'epoch_num': 1000,        # Número de épocas.\n",
        "    'lr': 1e-9,             # Taxa de aprendizado.\n",
        "    'weight_decay': 1e-9,   # Penalidade L2 (Regularização).\n",
        "    'batch_size': 10,        # Tamanho do batch.\n",
        "    'num_workers': 5,       # Quantos treinamentos simultâneos.\n",
        "}\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')"
      ],
      "metadata": {
        "id": "ZYc5KA8uoy7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregamento dos dados de atributos e rótulos\n",
        "atributos = torch.load('Tensores/atributos_random_tensor_n_aleatorio.pt')\n",
        "rotulos = torch.load('Tensores/rotulos_tensor_n_aleatorio.pt')\n"
      ],
      "metadata": {
        "id": "5DIMwotzo4T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_treino = ImagemTensor([atributos[:], rotulos[:]])\n",
        "# dataset_teste  = ImagemTensor([atributos[:], rotulos[:]])\n",
        "dataset_treino = ImagemTensor([atributos[:4200], rotulos[:4200]])\n",
        "dataset_teste  = ImagemTensor([atributos[4200:6000], rotulos[4200:6000]])"
      ],
      "metadata": {
        "id": "ED8qvy-qo-Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataloader_treino = DataLoader(dataset_treino,\n",
        "                               batch_size = args['batch_size'],\n",
        "                               shuffle=False,\n",
        "                               num_workers = args['num_workers'])\n",
        "\n",
        "dataloader_teste = DataLoader(dataset_teste,\n",
        "                              batch_size = args['batch_size'],\n",
        "                              shuffle=False,\n",
        "                              num_workers = args['num_workers'])"
      ],
      "metadata": {
        "id": "ThHBgVrfpDuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Regressor(nn.Module):\n",
        "\n",
        "  def __init__(self):   # Lembra que no init é obrigatório inicializar a super classe. Agora podemos fazer o que quisermos, que no caso do init é a definição da nossa arquitetura.\n",
        "    super(cnnclassifier, self).__init__()\n",
        "\n",
        "    #          ConvBlock 1\n",
        "    self.cnn       = nn.Conv2d(1, 256, kernel_size=3, stride=1, padding=0)\n",
        "    self.batch     = nn.BatchNorm2d(256)\n",
        "    self.relu      = nn.ReLU()\n",
        "    self.avgpool   = nn.AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
        "\n",
        "    #          ConvBlock 2\n",
        "    self.cnn2      = nn.Conv2d(256, 192, kernel_size=3, stride=1, padding=0)\n",
        "    self.batch2    = nn.BatchNorm2d(192)\n",
        "    self.relu2     = nn.ReLU()\n",
        "    self.maxpool2  = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "    #          ConvBlock 3\n",
        "    self.cnn3      = nn.Conv2d(192, 128, kernel_size=3, stride=1, padding=1)\n",
        "    self.batch3    = nn.BatchNorm2d(128)\n",
        "    self.relu3     = nn.ReLU()\n",
        "    self.maxpool3  = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
        "\n",
        "    #           Fully Connected\n",
        "    self.flatten    = nn.Flatten()\n",
        "    self.linear     = nn.Linear(100352,320)\n",
        "    self.relu4      = nn.ReLU()\n",
        "    self.linear2    = nn.Linear(320,1)\n",
        "\n",
        "  def forward(self, X):\n",
        "  #          CNN 1\n",
        "    out = self.cnn(X)\n",
        "    out = self.batch(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.avgpool(out)\n",
        "\n",
        "  #          CNN 2\n",
        "\n",
        "    out = self.cnn2(out)\n",
        "    out = self.batch2(out)\n",
        "    out = self.relu2(out)\n",
        "    out = self.maxpool2(out)\n",
        "\n",
        "  #         CNN 3\n",
        "\n",
        "    out = self.cnn3(out)\n",
        "    out = self.batch3(out)\n",
        "    out = self.relu3(out)\n",
        "    out = self.maxpool3(out)\n",
        "\n",
        "  #         FC1\n",
        "\n",
        "    out = self.flatten(out)\n",
        "    out = self.linear(out)\n",
        "    out = self.relu4(out)\n",
        "    out = self.linear2(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "net = CNN_Regressor()\n",
        "net = net.to(args['device'])\n"
      ],
      "metadata": {
        "id": "fNJYWyCv1SvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.device_count() >= 1:\n",
        "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
        "  modelparallel = nn.DataParallel(net)\n",
        "\n",
        "modelparallel.to(args['device'])\n",
        "print(modelparallel)"
      ],
      "metadata": {
        "id": "fVIQjuPKqITb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#criterion = nn.L1Loss().to(args['device'])\n",
        "criterion = nn.HuberLoss().to(args['device'])\n",
        "#criterion = nn.MSELoss().to(args['device'])\n",
        "\n",
        "#optimizer = optim.SGD(modelparallel.parameters(), lr=args['lr'], momentum=0.9, weight_decay=args['weight_decay'])\n",
        "optimizer = optim.Adam(modelparallel.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])"
      ],
      "metadata": {
        "id": "siDYyaCJqJXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader_treino, modelparallel, epoch):\n",
        "\n",
        "  # Training mode\n",
        "  modelparallel.train()\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  epoch_loss  = []\n",
        "  pred_list, rotulo_list = [], []\n",
        "  for batch in dataloader_treino:\n",
        "\n",
        "    dados, rotulo = batch\n",
        "\n",
        "    # Cast data in GPU\n",
        "    dados = dados.to(args['device'])\n",
        "    rotulo = rotulo.to(args['device'])\n",
        "\n",
        "    # Forward\n",
        "    ypred = modelparallel(dados)\n",
        "    loss = criterion(ypred, rotulo)\n",
        "    epoch_loss.append(loss.cpu().data)\n",
        "\n",
        "    _, pred = torch.max(ypred, axis=1)\n",
        "    pred_list.append(pred.cpu().numpy())\n",
        "    rotulo_list.append(rotulo.cpu().numpy())\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  epoch_loss = np.asarray(epoch_loss)\n",
        "  pred_list  = np.asarray(pred_list).ravel()\n",
        "  rotulo_list  = np.asarray(rotulo_list).ravel()\n",
        "\n",
        "\n",
        "\n",
        "  end = time.time()\n",
        "  print('#################### Train ####################')\n",
        "  print('Epoch %d, Loss: %.4f +/- %.4f, Time: %.2f' % (epoch, epoch_loss.mean(), epoch_loss.std(), end-start))\n",
        "\n",
        "\n",
        "  return epoch_loss.mean()"
      ],
      "metadata": {
        "id": "uztgDqE0qN2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(dataloader_teste, modelparallel, epoch):\n",
        "\n",
        "  # Evaluation mode\n",
        "  modelparallel.eval()\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  epoch_loss  = []\n",
        "  pred_list, rotulo_list = [], []\n",
        "  with torch.no_grad():\n",
        "    for batch in dataloader_teste:\n",
        "\n",
        "      dados, rotulo = batch\n",
        "\n",
        "      # Cast do dado na GPU\n",
        "      dados = dados.to(args['device'])\n",
        "      rotulo = rotulo.to(args['device'])\n",
        "\n",
        "      # Forward\n",
        "      ypred = net(dados)\n",
        "      loss = criterion(ypred, rotulo)\n",
        "      epoch_loss.append(loss.cpu().data)\n",
        "\n",
        "      _, pred = torch.max(ypred, axis=1)\n",
        "      pred_list.append(pred.cpu().numpy())\n",
        "      rotulo_list.append(rotulo.cpu().numpy())\n",
        "\n",
        "  epoch_loss = np.asarray(epoch_loss)\n",
        "\n",
        "  end = time.time()\n",
        "  print('********** Validate **********')\n",
        "  print('Epoch %d, Loss: %.4f +/- %.4f, Time: %.2f\\n' % (epoch, epoch_loss.mean(), epoch_loss.std(), end-start))\n",
        "  return epoch_loss.mean()\n",
        "\n",
        "train_losses, test_losses = [], []\n",
        "for epoch in range(args['epoch_num']):\n",
        "\n",
        "  # Train\n",
        "  train_losses.append(train(dataloader_treino, net, epoch))\n",
        "\n",
        "  # Validate\n",
        "  test_losses.append(validate(dataloader_teste, net, epoch))"
      ],
      "metadata": {
        "id": "knRkCkKeqcOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'Treino': train_losses, 'Teste': test_losses})"
      ],
      "metadata": {
        "id": "fI58iOhZqmyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel('ModelCNN/VGG_adam_ReLU30%dadosteste_SGD_e-9_pesoe-9_STACK.xlsx')"
      ],
      "metadata": {
        "id": "-yRCV27-qnpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(train_losses)\n",
        "plt.plot(test_losses, 'b', label = \"Teste\")\n",
        "plt.plot(train_losses, 'orange', label = \"Treino\")\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel(\"Épocas\")"
      ],
      "metadata": {
        "id": "EFib3qLxqn4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(train_losses)\n",
        "plt.plot(test_losses, 'b', label = \"Teste\")\n",
        "plt.plot(train_losses, 'orange', label = \"Treino\")\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel(\"Épocas\")"
      ],
      "metadata": {
        "id": "huZCPnP8qoFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def previsao_alexnet(ypred, rotulo):\n",
        "  y = (10**np.asarray(ypred).ravel())/1000\n",
        "  r = (10**np.asarray(rotulo).ravel())/1000\n",
        "  k_p = np.log10(y)\n",
        "  k_g = np.log10(r)\n",
        "\n",
        "  N = len(k_p)\n",
        "\n",
        "  soma = np.sum((k_p-k_g)**2)\n",
        "  raiz = np.sqrt(soma/N)\n",
        "\n",
        "  sigma = 10**(raiz)\n",
        "\n",
        "  return y, r, sigma"
      ],
      "metadata": {
        "id": "ENdW09arqoPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred, rotulo_pred, erro_sigma = previsao_alexnet(predicao, rotulo_lis )"
      ],
      "metadata": {
        "id": "wgSvSIGJqoa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplots(figsize = (6,4))\n",
        "\n",
        "sns.scatterplot(x = rotulo_pred,\n",
        "                y = y_pred)\n",
        "\n",
        "reta = pd.DataFrame({'x' : np.arange(100),\n",
        "                     'y' : np.arange(100)})\n",
        "sns.lineplot(data = reta,\n",
        "               x = 'x',\n",
        "               y = 'y')\n",
        "\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Permeabilidade Gás (mD)')\n",
        "plt.ylabel('Predição (mD)')\n",
        "plt.title('Permeabilidada gás x Predição')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bRB-EJLBr9Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_predicoes = pd.DataFrame({'Predição': y_pred, 'Rotulo': rotulo_pred, 'Erro sigma':erro_sigma})"
      ],
      "metadata": {
        "id": "K9DvYNXLsA2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save data\n",
        "df_predicoes.to_excel('ModelCNN/predição_TMM_ADAM_HUBER_tentativa2maxpool_ReLU30%dadosteste_e-9_STACK.xlsx')"
      ],
      "metadata": {
        "id": "LN8Sp1HFsBhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model\n",
        "torch.save(modelparallel, 'ModelCNN/model_TMM_pytorch_ADAM_30%dadosteste_HUBER_e-9_STACK.pt')"
      ],
      "metadata": {
        "id": "DNqyiG2NsG43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load model\n",
        "modelparallel_load = torch.load('ModelCNN/model_TMM_pytorch_ADAM_30%dadosteste_HUBER_e-9_STACK.pt')\n",
        "\n",
        "modelparallel_load.eval()"
      ],
      "metadata": {
        "id": "gCJqjcmJsHaH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}